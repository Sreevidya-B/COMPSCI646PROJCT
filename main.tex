
\documentclass[sigconf]{acmart}

\begin{document}

\title{Enhancing Standard Two-Stage IR systems with T5 and MMR for Biomedical Hypothesis Research}

\author{Stav Kinreich}
\email{skinreich@umass.edu}
\affiliation{
  \institution{University of Massachusetts Amherst}
  \city{Amherst}
  \state{Massachusetts}
  \country{USA}
}

\author{Sreevidya Bollineni}
\email{sreevidyabol@umass.edu}
\affiliation{
  \institution{University of Massachusetts Amherst}
  \city{Amherst}
  \state{Massachusetts}
  \country{USA}
}

\author{Wentao Ma}
\email{wentaoma@umass.edu}
\affiliation{
  \institution{University of Massachusetts Amherst}
  \city{Amherst}
  \state{Massachusetts}
  \country{USA}
}

\maketitle

\section{Problem Statement}
  Two-stage IR systems that utilize both sparse and dense models for document retrieval have gained immense popularity over recent years for their capabilities to retrieve relevant documents with high confidence and lower computational costs. These systems test their abilities by checking the relevance of the top retrieved documents against the given query, utilizing query-document relevance measures such as MAP, nDCG, F1, and more. While traditional two-stage IR systems perform well for general retrieval tasks, specific query tasks require more data analysis to procure a relevant list of documents that answer the task in context. Traditional models that rely purely on relevance metrics would not answer the task at hand that requires further context evaluation.
  
  In this paper, we consider the case of hypothesis testing and research, where a given query requires a more diverse set of retrieved documents than traditional two-stage IR systems produce. Specifically, a hypothesis research task requires the results to contain both supporting and contradicting relevant documents that are diverse in opinions and perspectives to construct a well-rounded research paper for the hypothesis. We define the task as follows: let $q$ represent a given query related to hypothesis testing, and let $D$ denote the large corpus of documents. Our goal is to retrieve two sets of documents, $D_{support}$ and $D_{contradict}$ such that $D_{support}\in D$ contains the document that supports hypothesis $q$ and $D_{contradict}\in D$ contains documents that contradict hypothesis $q$. The output of our system would be $D_{support}^*\cup D_{contradict}^*$, where the astrict of each dataset represents the top-k relevant documents from the set. we also note that $|D_{support}^*|\geq m$ and $|D_{contradict}^*|\geq m$ for some defined number $m$ to ensure a balance of opinions in the final list of documents. The key research question we introduce is: How can a retrieval system be designed to guarantee a balanced retrieval of supporting and contradictory evidence to assist in comprehensive hypothesis validation?

\section{Motivation}
In scientific fields like biomedicine, social sciences, and public policy, research rigor often depends on analyzing both supporting and opposing studies. For example, a researcher exploring the effects of a new drug might need to review studies that report both positive and negative outcomes to reach a well-rounded conclusion. However, locating both types of documents can be challenging and time-intensive when using traditional IR systems, which may lack the nuance to distinguish supportive versus opposing stances. In the context of traditional IR systems, the user is forced to develop multiple queries that would each procure a list of documents that support the given stance and perspective of the query to retrieve a balanced list of documents. Developing an IR system with built-in stance balancing would significantly enhance research efficiency and rigor, providing a valuable tool for hypothesis testing in fields that demand balanced viewpoints for critical decision-making. It is also important to note that all three writers of this paper have a personal interest in research in the biomedical field, and have in the past encountered the issue of balanced perspectives in the biomedical IR systems.

\section{Related work}

\section{Baselines}

\section{Datasets}

\section{Evaluation}

\section{References}

\end{document}
