# pip install rank-bm25
# conda install -c conda-forge sentencepiece -y
# pip install protobuf==3.20.0
# pip install sentencepiece==0.1.99
# pip install torch torchvision
# pip install transformers
# pip install transformers
# pip install sentencepiece

from transformers import T5Tokenizer, T5ForConditionalGeneration

model_name = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

import torch
from transformers import T5Tokenizer, T5ForSequenceClassification
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
from tqdm import tqdm
import os
from sklearn.metrics import ndcg_score
from rank_bm25 import BM25Okapi
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from nltk.util import ngrams

class T5DiversityCalculator:
    def __init__(self):
        """Initialize T5 model for stance classification"""
        print("Initializing T5 model...")
        self.tokenizer = T5Tokenizer.from_pretrained("t5-base")
        self.model = T5ForSequenceClassification.from_pretrained(
            "t5-base",
            num_labels=3  # support/contradict/neutral
        )
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        print(f"Model initialized on {self.device}")

    def get_stance_distribution_at_k(self, claim: str, documents: List[str], k_values: List[int]) -> Dict[int, np.ndarray]:
        """Get stance distribution for different k values"""
        all_stance_scores = []
        
        for doc in tqdm(documents, desc="Analyzing document stances"):
            input_text = f"Classify stance: claim: {claim} document: {doc}"
            inputs = self.tokenizer(
                input_text,
                return_tensors="pt",
                max_length=512,
                truncation=True,
                padding=True
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.softmax(outputs.logits, dim=-1)
                all_stance_scores.append(probabilities[0].cpu().numpy())

        all_stance_scores = np.array(all_stance_scores)
        
        k_distributions = {}
        for k in k_values:
            if k <= len(all_stance_scores):
                k_distributions[k] = all_stance_scores[:k]
        
        return k_distributions

    def calculate_inverse_simpson(self, stance_distribution: np.ndarray) -> float:
        """Calculate Inverse Simpson Index"""
        category_proportions = stance_distribution.mean(axis=0)
        simpson_index = np.sum(category_proportions ** 2)
        return 1 / simpson_index if simpson_index > 0 else 0

    def evaluate_diversity_at_k(self, claim: str, documents: List[str], k_values: List[int] = [5, 10, 20, 50, 100]) -> Dict:
        """Calculate diversity metrics for different k values"""
        k_distributions = self.get_stance_distribution_at_k(claim, documents, k_values)
        
        results = {}
        for k, distribution in k_distributions.items():
            category_proportions = distribution.mean(axis=0)
            inverse_simpson = self.calculate_inverse_simpson(distribution)
            
            results[k] = {
                'stance_support': float(category_proportions[0]),
                'stance_contradict': float(category_proportions[1]),
                'stance_neutral': float(category_proportions[2]),
                'inverse_simpson_index': float(inverse_simpson),
                'diversity_level': 'high' if inverse_simpson > 2 else 'medium' if inverse_simpson > 1.5 else 'low'
            }
            
        return results

def get_doc_ids_from_results(row: pd.Series, cord19_df: pd.DataFrame, bm25_results: List[Tuple[int, float]]) -> List[str]:
    """Get document IDs from BM25 results"""
    print(f"\nProcessing claim: {row['claim'][:50]}...")
    print(f"Topic ID: {row['topic_id']}")
    
    try:
        indices = [idx for idx, _ in bm25_results]
        doc_ids = [cord19_df.index[i] for i in indices]
        
        print(f"Found {len(doc_ids)} document IDs")
        print(f"First few doc IDs: {doc_ids[:5]}")
        
        return doc_ids
    except Exception as e:
        print(f"Error getting document IDs: {str(e)}")
        return []

def process_documents(doc_ids: List[str], cord19_df: pd.DataFrame) -> List[str]:
    """Process documents from CORD-19 dataset"""
    documents = []
    print(f"\nProcessing {len(doc_ids)} documents...")
    
    for doc_id in doc_ids:
        try:
            if doc_id in cord19_df.index:
                title = str(cord19_df.loc[doc_id, 'title']).strip()
                abstract = str(cord19_df.loc[doc_id, 'abstract']).strip()
                
                doc_text = f"{title} {abstract}".strip()
                if doc_text:
                    documents.append(doc_text)
                    if len(documents) <= 3:
                        print(f"Sample document {len(documents)}: {doc_text[:100]}...")
            else:
                print(f"Document ID {doc_id} not found in CORD-19 dataset")
        except Exception as e:
            print(f"Error processing document {doc_id}: {str(e)}")
            continue
    
    print(f"Successfully processed {len(documents)} valid documents")
    return documents

def bm25_retrieve(claim: str, corpus: List[str], top_n: int = 100) -> List[Tuple[int, float]]:
    """Perform BM25 retrieval"""
    print(f"\nPerforming BM25 retrieval for claim: {claim[:50]}...")
    
    tokenized_corpus = [doc.split() for doc in corpus]
    bm25 = BM25Okapi(tokenized_corpus)
    scores = bm25.get_scores(claim.split())
    
    top_indices = np.argsort(scores)[-top_n:][::-1]
    results = [(i, scores[i]) for i in top_indices]
    
    print(f"Retrieved {len(results)} documents")
    return results

# Helper function to calculate Self-BLEU for diversity
def calculate_self_bleu(documents, n_grams=4, k_values=[5, 10, 20, 50, 100]):
    results = {}
    for k in k_values:
        if k > len(documents):
            continue
        selected_docs = documents[:k]

        # Prepare hypotheses and references for BLEU calculation
        hypotheses = []
        list_of_references = []

        for i, target_doc in enumerate(selected_docs):
            references = selected_docs[:i] + selected_docs[i + 1:]
            hypotheses.append(target_doc.split())
            list_of_references.append([ref.split() for ref in references])

        # Calculate corpus-level BLEU score
        smoothing = SmoothingFunction().method1
        score = corpus_bleu(list_of_references, hypotheses, weights=(1.0/n_grams,) * n_grams, smoothing_function=smoothing)
        results[f"Self_BLEU@{k}"] = score

    return results

def calculate_metrics(row: pd.Series, calculator: T5DiversityCalculator, 
                     cord19_df: pd.DataFrame, k_values: List[int]) -> Dict:
    """Calculate metrics for a single claim"""
    claim = row['claim']
    print(f"\nProcessing claim: {claim[:50]}...")
    
    try:
        # Get CORD-19 documents
        cord19_docs = (
            cord19_df['title'].fillna('').astype(str) + " " + 
            cord19_df['abstract'].fillna('').astype(str)
        ).tolist()
        
        # Perform BM25 retrieval
        bm25_results = bm25_retrieve(claim, cord19_docs)
        
        # Get document IDs
        doc_ids = get_doc_ids_from_results(row, cord19_df, bm25_results)
        
        if not doc_ids:
            print("No document IDs found")
            return create_empty_metrics(row, k_values)
        
        # Get document contents
        documents = process_documents(doc_ids, cord19_df)
        
        if not documents:
            print("No valid documents found")
            return create_empty_metrics(row, k_values)
        
        # Calculate diversity metrics
        diversity_metrics = calculator.evaluate_diversity_at_k(claim, documents, k_values)

        # Calculate Self-BLEU scores
        self_bleu_scores = calculate_self_bleu(documents, k_values=k_values)
        
        # Build result dictionary
        result = {
            'claim': claim,
            'topic_id': row['topic_id']
        }
        
        # Add original metrics
        for k in k_values:
            result[f'ndcg@{k}'] = row[f'ndcg_cut_{k}']
            result[f'map@{k}'] = row[f'map_cut_{k}']
        
        # Add diversity metrics
        for k in k_values:
            if k in diversity_metrics:
                metrics = diversity_metrics[k]
                result.update({
                    f'stance_support@{k}': metrics['stance_support'],
                    f'stance_contradict@{k}': metrics['stance_contradict'],
                    f'stance_neutral@{k}': metrics['stance_neutral'],
                    f'inverse_simpson@{k}': metrics['inverse_simpson_index'],
                    f'diversity_level@{k}': metrics['diversity_level'],
                    f'Self_BLEU@{k}': self_bleu_scores.get(f'Self_BLEU@{k}', 0.0)
                })

        print(f"Successfully calculated metrics for k values: {k_values}")
        return result
    
    except Exception as e:
        print(f"Error calculating metrics: {str(e)}")
        return create_empty_metrics(row, k_values)

def create_empty_metrics(row: pd.Series, k_values: List[int]) -> Dict:
    """Create empty metrics for different k values"""
    result = {
        'claim': row['claim'],
        'topic_id': row['topic_id']
    }
    
    for k in k_values:
        result.update({
            f'ndcg@{k}': row[f'ndcg_cut_{k}'],
            f'map@{k}': row[f'map_cut_{k}'],
            f'stance_support@{k}': 0.0,
            f'stance_contradict@{k}': 0.0,
            f'stance_neutral@{k}': 0.0,
            f'inverse_simpson@{k}': 0.0,
            f'diversity_level@{k}': 'low',
            f'Self_BLEU@{k}': 0.0
        })
    
    return result


def main():
    """Main function"""
    # File paths
    bm25_results_file = "./results.csv"
    cord19_csv = "./processed_metadata.csv"
    output_file = "./diversity_results.csv"
    
    # K values - now including 5
    k_values = [5, 10, 20, 50, 100]
    
    print("Loading data...")
    bm25_results = pd.read_csv(bm25_results_file)
    print(f"Loaded {len(bm25_results)} results")
    
    cord19_df = pd.read_csv(cord19_csv)
    print(f"Loaded {len(cord19_df)} CORD-19 documents")
    
    cord19_df.set_index('cord_uid', inplace=True)
    
    # Initialize calculator
    calculator = T5DiversityCalculator()
    
    # Process claims
    all_results = []
    for idx, row in tqdm(bm25_results.iterrows(), total=len(bm25_results), desc="Processing claims"):
        metrics = calculate_metrics(row, calculator, cord19_df, k_values)
        all_results.append(metrics)
        
        if (idx + 1) % 10 == 0:
            print(f"\nProcessed {idx + 1}/{len(bm25_results)} claims")

    # Save results
    results_df = pd.DataFrame(all_results)
    results_df.to_csv(output_file, index=False)
    print(f"\nResults saved to: {output_file}")
    
    # Print statistics
    print("\nSummary Statistics:")
    for k in k_values:
        print(f"\nMetrics at k={k}:")
        print(f"Average Inverse Simpson Index@{k}: {results_df[f'inverse_simpson@{k}'].mean():.3f}")
        print(f"Average Stance Support@{k}: {results_df[f'stance_support@{k}'].mean():.3f}")
        print(f"Average Stance Contradict@{k}: {results_df[f'stance_contradict@{k}'].mean():.3f}")
        print(f"Average Stance Neutral@{k}: {results_df[f'stance_neutral@{k}'].mean():.3f}")
        print(f"Average Self_BLEU@{k}: {results_df[f'Self_BLEU@{k}'].mean():.3f}")
        
        if f'diversity_level@{k}' in results_df.columns:
            print(f"Diversity Level Distribution@{k}:")
            print(results_df[f'diversity_level@{k}'].value_counts())

if __name__ == "__main__":
    main()
